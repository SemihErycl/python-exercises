import numpy as np
import pandas as pd
# from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras import layers, models

# 1. Data Preparation
# Assume we have a DataFrame `df` with columns: ['price_change', 'volume', 'rsi', 'macd', ...]

# Simulating some sample data for demonstration
# np.random.seed(42)
# data_size = 1000
# df = pd.DataFrame({
#     'price_change': np.random.randn(data_size),
#     'volume': np.random.rand(data_size),
#     'rsi': np.random.rand(data_size) * 100,
#     'macd': np.random.randn(data_size),
#     # Add more technical indicators as needed
# })

testData = [5.2335, 5.234999999999999, 5.239, 5.2435, 5.243, 5.2455, 5.246499999999999, 5.2435, 5.2435, 5.2435, 5.243, 5.239, 5.2379999999999995, 5.237500000000001, 5.2385, 5.2379999999999995, 5.240500000000001, 5.2445, 5.2475000000000005, 5.247, 5.2455, 5.2455, 5.2455, 5.244, 5.244, 5.2435, 5.242, 5.245, 5.2455, 5.245, 5.243, 5.244999999999999, 5.25, 5.254, 5.2524999999999995, 5.253, 5.250500000000001, 5.254, 5.257, 5.258, 5.26, 5.2585, 5.2535, 5.2505, 5.2509999999999994, 5.2575, 5.2615, 5.2615, 5.263, 5.2645, 5.2635, 5.2615, 5.2620000000000005, 5.2655, 5.262499999999999, 5.2615, 5.2575, 5.254, 5.2565, 5.26, 5.2595, 5.2605, 5.257, 5.257, 5.256, 5.253, 5.2490000000000006, 5.2475000000000005, 5.2465, 5.245, 5.2415, 5.233499999999999, 5.2315000000000005, 5.2330000000000005, 5.233499999999999, 5.239, 5.2385, 5.2355, 5.2330000000000005, 5.2295, 5.2275, 5.2315000000000005, 5.231999999999999, 5.2330000000000005, 5.234500000000001, 5.2364999999999995, 5.2385, 5.2395, 5.2395, 5.2375, 5.239, 5.2395, 5.2415, 5.2445, 5.24, 5.239, 5.236, 5.241, 5.243, 5.244, 5.243, 5.24, 5.2405, 5.2395, 5.2379999999999995, 5.240500000000001, 5.243, 5.2425, 5.24, 5.241, 5.2415, 5.2415, 5.241, 5.241, 5.2405, 5.2425, 5.2435, 5.243, 5.241, 5.242, 5.2425, 5.2425, 5.242, 5.2435, 5.2455, 5.2465, 5.247999999999999, 5.248, 5.2490000000000006, 5.25, 5.252, 5.2515, 5.2545, 5.2545, 5.254, 5.255, 5.254, 5.250500000000001, 5.246, 5.243, 5.244, 5.2485, 5.243, 5.243, 5.240500000000001, 5.243, 5.243, 5.24, 5.239, 5.2415, 5.2435, 5.246, 5.2495, 5.2524999999999995, 5.252000000000001, 5.252000000000001, 5.2524999999999995, 5.25, 5.2475000000000005, 5.2435, 5.2445, 5.2445, 5.240500000000001, 5.2379999999999995, 5.242, 5.2475000000000005, 5.2509999999999994, 5.2524999999999995, 5.249499999999999, 5.248, 5.247, 5.243, 5.2435, 5.242, 5.245, 5.2435, 5.2435, 5.243, 5.2445, 5.247, 5.2475000000000005, 5.2455, 5.2465, 5.244, 5.242, 5.242, 5.2425, 5.2425, 5.2395, 5.239000000000001, 5.2364999999999995, 5.2335, 5.234, 5.2330000000000005, 5.233499999999999, 5.234, 5.236, 5.24, 5.237500000000001, 5.2415, 5.243, 5.2415, 5.240500000000001, 5.244, 5.243, 5.2445, 5.2425, 5.239, 5.2405, 5.241, 5.24, 5.2385, 5.2395, 5.2435, 5.247, 5.2475000000000005, 5.2475000000000005, 5.2490000000000006, 5.25, 5.2475000000000005, 5.2465, 5.2490000000000006, 5.2490000000000006, 5.25, 5.2485, 5.249499999999999, 5.2509999999999994, 5.2515, 5.2524999999999995, 5.2545, 5.255, 5.2555, 5.2575, 5.2620000000000005, 5.261, 5.2635, 5.265000000000001, 5.2669999999999995, 5.27, 5.2715, 5.2705, 5.268, 5.2695, 5.2705, 5.2705, 5.2665, 5.2684999999999995, 5.271, 5.274, 5.279999999999999, 5.284, 5.282, 5.2865, 5.288, 5.293, 5.297499999999999, 5.292, 5.2805, 5.2765, 5.2715, 5.263, 5.2585, 5.252000000000001, 5.2435, 5.2415, 5.2355, 5.2330000000000005, 5.221, 5.2015, 5.18, 5.183, 5.1675, 5.1845, 5.198, 5.1965, 5.1935, 5.1925, 5.198, 5.2085, 5.2055, 5.2075, 5.2035, 5.205, 5.2105, 5.215999999999999, 5.2175, 5.2115, 5.211, 5.212999999999999, 5.214, 5.2115, 5.2044999999999995, 5.2010000000000005, 5.196, 5.1905, 5.192, 5.195, 5.191000000000001, 5.183, 5.1850000000000005, 5.1845, 5.1835, 5.1775, 5.183, 5.1835, 5.186, 5.1850000000000005, 5.187, 5.1915, 5.1899999999999995, 5.1884999999999994, 5.1855, 5.1835, 5.185499999999999, 5.1835, 5.181, 5.181, 5.1805, 5.1795, 5.1855, 5.191, 5.1895, 5.1905, 5.196, 5.1935, 5.1955, 5.1945, 5.192500000000001, 5.191, 5.1885, 5.189, 5.189500000000001, 5.1899999999999995, 5.1899999999999995, 5.191, 5.191, 5.1905, 5.1945, 5.195, 5.196, 5.196, 5.198, 5.1945, 5.199, 5.1965, 5.192500000000001, 5.1845, 5.1875, 5.1884999999999994, 5.1945, 5.1945, 5.193, 5.1925, 5.1965, 5.1965, 5.195, 5.191, 5.189500000000001, 5.189500000000001, 5.1925, 5.1899999999999995, 5.1899999999999995, 5.193, 5.196, 5.194, 5.191000000000001, 5.1865000000000006, 5.186, 5.189500000000001, 5.1895, 5.1865000000000006, 5.183999999999999, 5.183, 5.181, 5.177, 5.176, 5.173, 5.175, 5.175, 5.1765, 5.183999999999999, 5.1815, 5.1815, 5.1754999999999995, 5.177, 5.176, 5.1754999999999995, 5.172499999999999, 5.1754999999999995, 5.1754999999999995, 5.1765, 5.178, 5.1805, 5.183, 5.186999999999999, 5.1875, 5.1905, 5.192, 5.188, 5.182, 5.183, 5.184, 5.1884999999999994, 5.1884999999999994, 5.186, 5.184, 5.184, 5.1845, 5.1825, 5.1805, 5.178, 5.1765, 5.173, 5.1725, 5.175, 5.1715, 5.1665, 5.1605, 5.1625, 5.1625, 5.1625, 5.161, 5.156, 5.1445, 5.1465, 5.1525, 5.156, 5.1555, 5.150499999999999, 5.1545000000000005, 5.154999999999999, 5.1579999999999995, 5.16, 5.1594999999999995, 5.162, 5.165, 5.161, 5.1625, 5.1705000000000005, 5.1690000000000005, 5.1695, 5.167999999999999, 5.167999999999999, 5.16, 5.156000000000001, 5.1585, 5.1594999999999995, 5.164999999999999, 5.1675, 5.1690000000000005, 5.17, 5.1745, 5.1739999999999995, 5.1715, 5.172000000000001, 5.1685, 5.169499999999999, 5.1665, 5.1655, 5.1675, 5.170999999999999, 5.169499999999999, 5.17, 5.1739999999999995, 5.173, 5.1705000000000005, 5.1705000000000005, 5.175, 5.176500000000001, 5.173, 5.172499999999999, 5.170999999999999, 5.1705000000000005, 5.1655, 5.165, 5.167, 5.172000000000001, 5.173, 5.17, 5.166, 5.1675, 5.164, 5.1645, 5.1645, 5.1625, 5.163, 5.1665, 5.170999999999999, 5.172000000000001, 5.1715, 5.173, 5.1739999999999995, 5.175, 5.1735, 5.176, 5.18, 5.176, 5.177, 5.177, 5.1795, 5.1765, 5.179, 5.18, 5.177, 5.184, 5.186999999999999, 5.181, 5.1785, 5.1815, 5.181, 5.18, 5.1775, 5.1795, 5.1775, 5.1795, 5.18, 5.183, 5.1835, 5.186, 5.1875, 5.186999999999999, 5.1855, 5.183, 5.1845, 5.183, 5.1815, 5.1850000000000005, 5.1875, 5.1885, 5.191, 5.1925, 5.189500000000001, 5.193, 5.196, 5.194, 5.1895, 5.189, 5.184, 5.186, 5.1899999999999995, 5.186, 5.183, 5.179, 5.1735, 5.1754999999999995, 5.1775, 5.177, 5.1785, 5.181, 5.18, 5.1815, 5.182, 5.181, 5.181, 5.18, 5.176, 5.1785, 5.183, 5.184, 5.186, 5.186, 5.1850000000000005, 5.185499999999999, 5.188000000000001, 5.183, 5.1805, 5.182, 5.181, 5.1805, 5.1825, 5.1850000000000005, 5.183, 5.1795, 5.173, 5.170999999999999, 5.1705000000000005, 5.173, 5.172000000000001, 5.1705000000000005, 5.1695, 5.17, 5.1635, 5.160500000000001, 5.154, 5.154999999999999, 5.153499999999999, 5.15, 5.144, 5.132, 5.1235, 5.112, 5.1145, 5.109999999999999, 5.0825, 5.055, 5.0625, 5.0565, 5.0205, 5.0169999999999995, 5.008, 5.0095, 5.003, 4.976, 4.974, 4.98, 4.9785, 4.978, 4.9704999999999995, 4.9825, 4.987500000000001, 4.9785, 4.979, 4.99, 4.9925, 4.9965, 4.996, 4.9905, 4.998, 5.0085, 5.0075, 5.0025, 4.998, 4.981, 4.9815000000000005, 4.9805, 4.9815000000000005, 4.985, 4.982, 4.9705, 4.952, 4.944, 4.949, 4.955500000000001, 4.9635, 4.9655000000000005, 4.964, 4.9615, 4.964, 4.970000000000001, 4.967499999999999, 4.963, 4.96, 4.9575, 4.9544999999999995, 4.948, 4.9435, 4.938000000000001, 4.9395, 4.93, 4.938000000000001, 4.946999999999999, 4.945, 4.946, 4.954, 4.952999999999999, 4.943, 4.9415, 4.9425, 4.936, 4.9395, 4.944000000000001, 4.9415, 4.9345, 4.929, 4.9215, 4.9285, 4.9350000000000005, 4.9350000000000005, 4.930999999999999, 4.9335, 4.9315, 4.933999999999999, 4.939, 4.9415, 4.943, 4.9399999999999995, 4.9365000000000006, 4.939, 4.9315, 4.9295, 4.9215, 4.921, 4.9275, 4.936, 4.938000000000001, 4.936999999999999, 4.936999999999999, 4.945, 4.95, 4.948, 4.9544999999999995, 4.957000000000001, 4.9544999999999995, 4.9495000000000005, 4.943, 4.942, 4.9405, 4.938, 4.943, 4.942, 4.936, 4.9355, 4.932, 4.9325, 4.933, 4.9275, 4.9265, 4.920999999999999, 4.92, 4.927, 4.9315, 4.930999999999999, 4.9254999999999995, 4.92, 4.914, 4.916499999999999, 4.9125, 4.8975, 4.884, 4.890000000000001, 4.8919999999999995, 4.899, 4.901, 4.9, 4.898, 4.8919999999999995, 4.893, 4.885, 4.8855, 4.8895, 4.8875, 4.89, 4.881, 4.880000000000001, 4.872, 4.876, 4.88, 4.8795, 4.888, 4.885999999999999, 4.8835, 4.8735, 4.8725000000000005, 4.867, 4.8635, 4.872, 4.8795, 4.889, 4.8915, 4.899, 4.906, 4.909000000000001, 4.917, 4.916, 4.9185, 4.923500000000001, 4.9275, 4.9245, 4.927, 4.9285, 4.93, 4.9239999999999995, 4.922000000000001, 4.9190000000000005, 4.9215, 4.9165, 4.917, 4.9215, 4.9245, 4.9295, 4.928, 4.9275, 4.933, 4.936, 4.9350000000000005, 4.9365000000000006, 4.9350000000000005, 4.9285, 4.9285, 4.9275, 4.929, 4.929, 4.928, 4.9325, 4.9315, 4.9275, 4.9254999999999995, 4.9265, 4.928, 4.923, 4.923500000000001, 4.9295, 4.928, 4.923500000000001, 4.9239999999999995, 4.921, 4.9215, 4.926, 4.9245, 4.922, 4.92, 4.92, 4.925, 4.9254999999999995, 4.9254999999999995, 4.9215, 4.9275, 4.93, 4.9254999999999995, 4.9245, 4.9254999999999995, 4.929, 4.932, 4.932499999999999, 4.9350000000000005, 4.9365, 4.936, 4.9384999999999994, 4.942, 4.9375, 4.9365000000000006, 4.9350000000000005, 4.935499999999999, 4.9365000000000006, 4.9405, 4.942, 4.943, 4.9415, 4.932, 4.928, 4.923, 4.92, 4.915, 4.9155, 4.9185, 4.923, 4.9239999999999995, 4.9190000000000005, 4.9190000000000005, 4.92, 4.9215, 4.922000000000001, 4.9275, 4.927, 4.93, 4.934, 4.9345, 4.9350000000000005, 4.9295, 4.925, 4.926, 4.9275, 4.922499999999999, 4.9195, 4.917, 4.9105, 4.9115, 4.9145, 4.912000000000001, 4.9115, 4.907, 4.906, 4.9085, 4.913, 4.915, 4.915, 4.919499999999999, 4.92, 4.922000000000001, 4.927, 4.9239999999999995, 4.9245, 4.9235, 4.927, 4.9285, 4.9285, 4.932499999999999, 4.9285, 4.926, 4.929, 4.9239999999999995, 4.9175, 4.9239999999999995, 4.9225, 4.923, 4.926, 4.928, 4.927, 4.9315, 4.936, 4.9385, 4.937, 4.934, 4.935499999999999, 4.939, 4.9445, 4.945, 4.9495000000000005, 4.952, 4.9505, 4.9465, 4.947, 4.95, 4.9515, 4.9515, 4.9515, 4.952, 4.9475, 4.9495000000000005, 4.9565, 4.9559999999999995, 4.959, 4.9565, 4.959, 4.9605, 4.9575, 4.9615, 4.965, 4.964, 4.9670000000000005, 4.9704999999999995, 4.9715, 4.9725, 4.976, 4.9855, 5.0145, 5.009, 5.0125, 5.0115, 5.0115, 5.0065, 4.996499999999999, 4.9879999999999995, 4.986, 4.9855, 4.984999999999999, 4.9935, 4.9975000000000005, 4.993, 4.9895, 4.986000000000001, 4.981, 4.985, 4.9830000000000005, 4.98, 4.98, 4.981, 4.975, 4.973, 4.977, 4.977, 4.9719999999999995, 4.9735, 4.976, 4.974, 4.975, 4.9695, 4.966, 4.968, 4.9715, 4.976, 4.9704999999999995, 4.964499999999999, 4.965, 4.966, 4.963, 4.964499999999999, 4.9585, 4.955500000000001, 4.9544999999999995, 4.957, 4.9625, 4.965, 4.965, 4.9595, 4.96, 4.964499999999999, 4.968500000000001, 4.9725, 4.977, 4.9775, 4.9735, 4.973, 4.9785, 4.985, 4.9879999999999995, 4.984, 4.979, 4.9785, 4.9795, 4.9785, 4.9805, 4.976, 4.9725, 4.9725, 4.973, 4.9719999999999995, 4.975, 4.9825, 4.9830000000000005]
downScale = 6
for i in range(len(testData)):
    testData[i] = testData[i] / downScale

# Function to create sequences of data
def create_sequences(data, lookback=200):
    sequences = []
    targets = []
    for i in range(len(data) - lookback - 30):
        sequences.append(data[i:i + lookback])
        targets.append(max(data[i + lookback:i + lookback + 30]))
    return np.array(sequences), np.array(targets)

lookback = 200
sequences, targets = create_sequences(testData, lookback=lookback)

# Train/Test Split
train_size = int(len(sequences) * 0.8)
X_train, X_test = sequences[:train_size], sequences[train_size:]
y_train, y_test = targets[:train_size], targets[train_size:]

# Reshape the input data to be 3D as required by LSTM
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))  # Adding the feature dimension
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))


# Normalize/Scale the data
# scaler = MinMaxScaler()
# X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
# X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

# 2. Model Creation

# Create the model
model = models.Sequential([
    layers.Input(shape=(lookback, X_train.shape[-1])),  # Input layer
    layers.LSTM(64, activation='relu', return_sequences=True),  # LSTM layer with 64 units
    layers.LSTM(32, activation='relu'),  # Another LSTM layer with 32 units
    layers.Dense(32, activation='relu'),  # Fully connected layer
    layers.Dropout(0.2),  # Dropout layer to prevent overfitting
    layers.Dense(1)  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='mean_absolute_error',
              metrics=['mean_absolute_error', "precision", "recall"])

# 3. Model Training
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# 4. Model Evaluation
evaluation = model.evaluate(X_test, y_test)
print(f"Test MAE: {evaluation[1]}")
print(f"Test Precision: {evaluation[2]}")
print(f"Test Recall: {evaluation[3]}")

# 5. Predicting on New Data
# Example of predicting on a new sequence
new_sequence = X_test[0].reshape(1, lookback, X_train.shape[-1])
probability = model.predict(new_sequence)
print(f"Highest price in  next 30 mins: {probability[0][0]}")
